---
title: "Particle MCMC"
author: "Sam Abbott"
date: "04/10/2016"
output: html_document
---


```{r load packages, echo=TRUE, message=FALSE, include=FALSE}
#install.packages('tidyverse')
library(tidyverse)

# install.packages("devtools")
library(devtools)

# install_github("sbfnk/fitR@fitcourse_2016")
library(fitR)

#install.packages('coda')
library(coda)
```


#Introduction

aim is to fit a stochastic model using particle mcmc metropolis hastings algorithm

- we must explore the parameter space efficiently 
- evaluate the likelihodd at a given parameter

Session aims:

1. code a particle filter
2. learn how to calibrate the number of particles
3. fit the stochastic seIt$L model to the Tristan da Cunha outbreak with a pMCMC

#Code a particle filter

```{r particle MCMC, echo=TRUE, message=FALSE}
# This is a function that takes four parameters:
# - fitmodel: a fitmodel object
# - theta: named numeric vector. Values of the parameters for which the marginal log-likelihood is desired.
# - init.state: named numeric vector. Initial values of the state variables.
# - data: data frame. Observation times and observed data.
# The function returns the value of the marginal log-likelihood
my_particleFilter <- function(fitmodel, theta, init.state, data, n.particles) {

    ## Initialisation of the algorithm

    # Marginal log-likelihood is set to 0 and will be updated during the filtering steps
    margLogLike <- 0

    # Particle states can be stored in a list
    state.particles  <- rep(list(init.state), n.particles)

    # Weight: initially equal for all the particles 
    # particle weight can be stored in a vector
    weight.particles <- rep(1/n.particles, length = n.particles)

    # Initialise time variable
    current.time <- 0

    ## Loop over observation times: resample, propagate, weight
    for(i in seq_len(nrow(data))){

        # Extract next data point (must be a vector)
        data.point <- unlist(data[i, ])
        next.time <- data.point["time"]

        # Resample particles according to their weights. 
        # You can use the `sample` function of R
        # (normalisation of the weights is done in the function)
        index.resampled <- sample(x = n.particles,
                                  size = n.particles,
                                  replace = TRUE,
                                  prob = weight.particles)
        state.particles <- state.particles[index.resampled]

        ## Loop over particles: propagate and weight
        for(p in 1:n.particles){

            # Extract current state of the particle 
            current.state.particle <- state.particles[[p]]

            # Propagate the particle from current observation time 
            # to the next one using the function `fitmodel$simulate`
            traj <- fitmodel$simulate(theta = theta,
                                      init.state = current.state.particle,
                                      times = c(current.time,next.time))

            # Extract state of the model at next observation time
            # Also make sure that model.point is a vector
            model.point <- unlist(traj[2,fitmodel$state.names])

            # Weight the particle with the likelihood of the observed 
            # data point using the function `fitmodel$dPointObs`
            weight.particles[p] <-
                fitmodel$dPointObs(data.point = data.point,
                                       model.point = model.point,
                                       theta = theta)

            # Update state of the p particle
            state.particles[[p]] <- model.point

        }

        # Increment time
        current.time <- next.time

        ## Increment the marginal log-likelihood
        # Add the log of the mean of the particles weights
        margLogLike <- margLogLike + log(mean(weight.particles))
    }

    ## Return marginal log-likelihood
    return(margLogLike)

}
```

#Run a particle filter

```{r run particle MCMC, echo=TRUE, message=FALSE}

# load SEIT4L_stoch
data(SEIT4L_stoch)

# load data
data(FluTdC1971)

# theta close to the mean posterior estimate of the deterministic SEIT4L
# model
theta <- c(R0 = 7, D_lat = 1, D_inf = 4, alpha = 0.5, D_imm = 10, rho = 0.65)

# init state as before
init.state <- c(S = 279, E = 0, I = 2, T1 = 3, T2 = 0, T3 = 0, T4 = 0, L = 0, 
    Inc = 0)

# run the particle filter with 20 particles
my_particleFilter(SEIT4L_stoch, theta, init.state, data = FluTdC1971, n.particles = 20)
```

#calibrate an optimal number of particles

```{r calibrate particles, eval=FALSE, echo=TRUE, message=FALSE}
# load fitmodel, data and define init.state
data(SEIT4L_stoch)
data(FluTdC1971)

init.state <- c("S" = 279,"E" = 0,"I" = 2,"T1" = 3,"T2" = 0, "T3" = 0, "T4" = 0,"L" = 0,"Inc" = 0) 

# pick a theta close to the mean posterior estimate of the deterministic fit
theta <- c("R0" = 7, "D_lat" = 1 , "D_inf" = 4, "alpha" = 0.5, "D_imm" = 10, "rho" = 0.65)

# vector of number of particles to test
test.n.particles <- seq(100, 500, 100)

# number of replicates 
n.replicates <- 10

# vector and data frame of results
sample.log.like <- vector("numeric", length = n.replicates)
res <- data.frame()

for(n.particles in test.n.particles){

    # start measuring time
    start.time  <- Sys.time()
    for(i in 1:n.replicates){
        # one Monte-Carlo estimate of the log-likelihood
            sample.log.like[i] <- my_particleFilter(SEIT4L_stoch, theta,
                                                    init.state, FluTdC1971,
                                                    n.particles)
    }
    # end measuring time
    end.time  <- Sys.time()

    # keep only replicate with finite log-likelihood to be able to compute the mean and sd
    # this give us the proportion of replicates with particle depletion.
    sample.finite.log.like <- sample.log.like[is.finite(sample.log.like)]

    ans <- c(mean = mean(sample.finite.log.like), 
                 sd = sd(sample.finite.log.like), 
                 prop.depletion = 1-length(sample.finite.log.like)/length(sample.log.like), 
                 time = end.time - start.time)

    res <- rbind(res, t(ans))
}
```


```{r set up pMCMC, echo=TRUE, message=FALSE}
# the fitmodel
data(SEIT4L_stoch)

# wrapper for posterior
my_posteriorSto <- function(theta){

    my_fitmodel <- SEIT4L_stoch
    my_init.state <- c("S" = 279,"E" = 0,"I" = 2,"T1" = 3,"T2" = 0, "T3" = 0, "T4" = 0,"L" = 0,"Inc" = 0) 

    my_n.particles <- 400 
    # you can reduce the number of particles if your pMCMC is too slow

    return(logPosterior(fitmodel = my_fitmodel,
                            theta = theta,
                            init.state = my_init.state,
                            data = FluTdC1971,
                            margLogLike = my_particleFilter,
                            n.particles = my_n.particles))

}

# load results of deterministic fit
data(mcmc_TdC_deter_longRun)

# Let's use the first trace only, no need to burn or thin
trace <- mcmc_SEITL_infoPrior_theta1$trace

# we will start the pMCMC at the mean posterior estimate
# of the deterministic fit
init.theta <- colMeans(trace[SEIT4L_stoch$theta.names])

# and we take the empirical covariance matrix for the 
# Gaussian kernel proposal
covmat <- mcmc_SEITL_infoPrior_theta1$covmat.empirical

# lower and upper limits of each parameter
lower <- c(R0 = 0, D_lat = 0 , D_inf = 0, alpha = 0, D_imm = 0, rho = 0)
upper <- c(R0 = Inf, D_lat = Inf , D_inf = Inf, alpha = 1, D_imm = Inf, rho = 1)

# number of iterations for the MCMC
n.iterations <- 50 # just a few since it takes quite a lot of time

# Here we don't adapt so that we can check the acceptance rate of the empirical covariance matrix
adapt.size.start <- 100
adapt.size.cooling <- 0.99
adapt.shape.start <- 100
```

```{r run pMCMC, echo=TRUE, message=FALSE, cache=TRUE}
# run the pMCMC
my_pMCMC <- mcmcMH(target = my_posteriorSto,
                   init.theta = init.theta,
                   covmat = covmat,
                   limits = list(lower = lower,upper = upper),
                   n.iterations = n.iterations,
                   adapt.size.start = adapt.size.start,
                   adapt.size.cooling = adapt.size.cooling,
                   adapt.shape.start = adapt.shape.start)
```

#Analysis with 50 particles: pMCMC

```{r load run 50, echo=TRUE, message=FALSE, cache=TRUE}
# load traces
data(pmcmc_SEIT4L_infoPrior_n50)

# combine into a `mcmc.list` object
trace <- mcmc.list(lapply(pmcmc_SEIT4L_infoPrior_n50, function(chain) {
    mcmc(chain$trace)
}))

# acceptance rate is below the optimal 23%
1 - rejectionRate(trace)


# accordingly, the combined ESS is a bit low
effectiveSize(trace)


# Let's have a look at the traces
xyplot(trace)

# this can take some time as we have 5 chains
plotESSBurn


# Actually, it doesn't seem necessary to burn.  What about autocorrelation
acfplot(x = trace, lag.max = 50)


# There is substantial autocorrelation but we can't thin too much since the
# chains are quite short.  So let's keep 1 iteration every 20
trace.thin.n50 <- burnAndThin(trace, thin = 20)

# Finally we can plot the posterior density
densityplot(x = trace.thin.n50)
```

#Analysis with 400 particles: pMCMC

```{r load run 400, echo=TRUE, message=FALSE, cache=TRUE}
# load traces
data(pmcmc_SEIT4L_infoPrior_n400)

# combine into a `mcmc.list` object
trace <- mcmc.list(lapply(pmcmc_SEIT4L_infoPrior_n400, function(chain) {
    mcmc(chain$trace)
}))

# acceptance rate is near optimal
1 - rejectionRate(trace)


# Note that the combined ESS is 2 times higher than with 50 particles
effectiveSize(trace)


# Let's have a look at the traces
xyplot(trace)

# Actually, it looks like no burning is needed:
plotESSBurn(trace)

# What about autocorrelation?
acfplot(x = trace, lag.max = 50)


# Autocorrelation decreases much more quickly than with 50 particles Let's
# keep 1 iteration every 20
trace.thin.n400 <- burnAndThin(trace, thin = 20)

# Let's plot the posterior densities
densityplot(x = trace.thin.n400)

#compare 400 and 50 particle runs
plotPosteriorDensity(list(n50 = trace.thin.n50, n400 = trace.thin.n400))
```

#Stochastic versus deterministic fit

```{r compare stoch vs det, echo=TRUE, message=FALSE, cache=TRUE}

# load, burn and thin the deterministic fit

# create mcmc object
trace1 <- mcmc(mcmc_SEIT4L_infoPrior_theta1$trace)
trace2 <- mcmc(mcmc_SEIT4L_infoPrior_theta2$trace)

# combine in a mcmc.list
trace <- mcmc.list(trace1, trace2)

# burn and thin as the chain with uniform prior (see above sections)
trace.deter <- burnAndThin(trace, burn = 5000, thin = 40)

# compare posterior density
plotPosteriorDensity(list(deter = trace.deter, sto = trace.thin.n400))
```

- differences in the log likelihood indicate that the stochastic models fits much better
- there is a large discrepancy in the estimates for $R_0$ and $D_imm$
- quantify this be comparing the DIC of both models

```{r calc DIC stoch, echo=TRUE, message=FALSE, cache=TRUE}
# combine all traces in a data frame
trace.combined <- ldply(trace.thin.n400)

# take the mean of theta
theta.bar <- colMeans(trace.combined[SEIT4L_stoch$theta.names])
print(theta.bar)


# compute its log-likelihood
init.state <- c(S = 279, E = 0, I = 2, T1 = 3, T2 = 0, T3 = 0, T4 = 0, L = 0, 
    Inc = 0)
log.like.theta.bar <- my_particleFilter(SEIT4L_stoch, theta.bar, init.state, 
    data = FluTdC1971, n.particles = 400)
print(log.like.theta.bar)

log.like.theta.bar.deter <- dTrajObs(SEIT4L_deter, theta.bar, init.state, data = FluTdC1971, 
    log = TRUE)
print(log.like.theta.bar.deter)


# and its deviance
D.theta.bar <- -2 * log.like.theta.bar
print(D.theta.bar)


# the effective number of parameters
p.D <- var(-2 * trace.combined$log.likelihood)/2
print(p.D)


# and finally the DIC
DIC <- D.theta.bar + 2 * p.D
print(DIC)

```

In the previous section we found thatthe DIC of the stochastic model was 265 therefore the stochastic model should be strongle preferred

```{r mean of posterior, echo=TRUE, message=FALSE, cache=TRUE}
# take the mean posterior estimates of the deterministic model
x <- summary(trace.deter)
theta.bar.deter <- x$statistics[SEIT4L_deter$theta.names, "Mean"]
```

```{r plot the posterior fit code, echo=TRUE, message=FALSE,eval=FALSE}
plotFit(SEIT4L_stoch, theta.bar, init.state, data = FluTdC1971, n.replicates = 1000)
```

```{r plot the posterior fit eval, echo=TRUE, message=FALSE,include=FALSE}
plotFit(SEIT4L_stoch, theta.bar, init.state, data = FluTdC1971, n.replicates = 1000)
```

```{r plot the posterior fit code det, echo=TRUE, message=FALSE,eval=FALSE}
plotFit(SEIT4L_deter, theta.bar, init.state, data = FluTdC1971, n.replicates = 1000)
```

```{r plot the posterior fit eval det, echo=TRUE, message=FALSE,include=FALSE}
plotFit(SEIT4L_deter, theta.bar, init.state, data = FluTdC1971, n.replicates = 1000)
```

